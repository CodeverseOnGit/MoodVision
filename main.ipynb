{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ffa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision matplotlib scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2ce7581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from PIL import Image\n",
    "\n",
    "DATA_DIR = \"Data\"         # expects Data/<five different emotions folders>/\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 12\n",
    "LR = 1e-4\n",
    "NUM_WORKERS = 4\n",
    "MODEL_PATH = \"best_emotion_resnet50.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "VAL_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e239b662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5181a2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Angry', 'Fear', 'Happy', 'Sad', 'Suprise']\n",
      "Number of training samples: 47280\n",
      "Number of validation samples: 11819\n"
     ]
    }
   ],
   "source": [
    "# Dataset split\n",
    "full_dataset = datasets.ImageFolder(DATA_DIR, transform=train_transforms)\n",
    "num_val = int(len(full_dataset) * VAL_SPLIT)\n",
    "num_train = len(full_dataset) - num_val\n",
    "train_ds, val_ds = random_split(full_dataset, [num_train, num_val])\n",
    "\n",
    "# Update validation dataset transforms\n",
    "val_ds.dataset.transform = val_transforms\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "class_names = full_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"Number of training samples: {len(train_ds)}\")\n",
    "print(f\"Number of validation samples: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e579a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# models\n",
    "model = models.resnet50(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True  # fine-tune all layers; set false to freeze backbone\n",
    "\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, num_classes)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a360299",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # Loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR) # Optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) # Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5a333ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation helper\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data).item()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "    acc = running_corrects / len(loader.dataset)\n",
    "    return avg_loss, acc, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5450dc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/12]:  24%|██▎       | 350/1478 [30:48<1:26:50,  4.62s/it, loss=0.76] "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "best_acc = 0.0\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\", leave=False)\n",
    "    for inputs, labels in loop:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_loss = running_loss / len(train_ds)\n",
    "    train_acc = running_corrects / len(train_ds)\n",
    "    val_loss, val_acc, _, _ = evaluate(model, val_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}  time: {epoch_time:.1f}s  train_loss: {train_loss:.4f}  train_acc: {train_acc:.4f}  val_loss: {val_loss:.4f}  val_acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save({\n",
    "            \"epoch\": epoch+1,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"class_names\": class_names,\n",
    "            \"val_acc\": best_acc\n",
    "        }, MODEL_PATH)\n",
    "        print(f\"Saved best model to {MODEL_PATH} (val_acc={best_acc:.4f})\")\n",
    "print(\"Training complete. Best val acc:\", best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d29958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final evaluation\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "val_loss, val_acc, all_preds, all_labels = evaluate(model, val_loader)\n",
    "print(\"Final val loss:\", val_loss, \"val acc:\", val_acc)\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# 8. Plot training curves\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6990304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img_path, model, topk=1):\n",
    "    model.eval()\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    t = val_transforms(img).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(t)\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        top_probs, top_idx = probs.topk(topk, dim=1)\n",
    "    idx_to_class = {v:k for k,v in full_dataset.class_to_idx.items()}\n",
    "    results = [(idx_to_class[int(i)], float(p)) for p, i in zip(top_probs[0], top_idx[0])]\n",
    "    return results\n",
    "\n",
    "print(predict_image(\"Data/Angry/77.png\", model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a83e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding webcam functionality \n",
    "# Real time emotion detection\n",
    "\n",
    "# Load model\n",
    "model = models.resnet50()\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 5)  # 5 emotions\n",
    "model.load_state_dict(torch.load(\"best_emotion_resnet50.pth\", map_location=\"cpu\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb4163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "classes = ['Angry', 'Fear', 'Happy', 'Sad', 'Surprised']\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # 0 = default webcam\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # Convert frame to model input\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = transform(img).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(img_pil)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        emotion = classes[pred.item()]\n",
    "\n",
    "    # Draw text on the video feed\n",
    "    cv2.putText(frame, emotion, (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "\n",
    "    cv2.imshow(\"Emotion Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
